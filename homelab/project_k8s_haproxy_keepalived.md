# Project - K8s Cluster using HAProxy Load Balancer and Keepalived

## Table of Contents
* [Set up the Environment](#set-up-the-environment) 
    * [Spin up VMs](#spin-up-vms) 
    * [Install K8s](#install-k8s) 
    * [Initialize k8s Cluster](#initialize-k8s-cluster) 
    * [Install a CNI plugin (flannel)](#install-a-cni-plugin-flannel) 
    * [Join the Worker Nodes](#join-the-worker-nodes) 
    * [Deploy a Test App](#deploy-a-test-app) 
    * [Check which NodePort was Assigned](#check-which-nodeport-was-assigned) 
* [Set up two HAProxy Nodes](#set-up-two-haproxy-nodes) 
    * [Configure HAProxy](#configure-haproxy) 
* [Set up Keepalived for a Virtual IP (VIP)](#set-up-keepalived-for-a-virtual-ip-vip) 
    * [Install Keepalived on the HAProxy Nodes](#install-keepalived-on-the-haproxy-nodes) 
    * [Configure Keepalived](#configure-keepalived) 
        * [First HAProxy Node's Keepalived Configuration](#first-haproxy-nodes-keepalived-configuration) 
        * [Second HAProxy Node's Keepalived Configuration](#second-haproxy-nodes-keepalived-configuration) 
    * [Start/Restart Keepalived](#startrestart-keepalived) 
* [Test the Virtual IP and Failover](#test-the-virtual-ip-and-failover) 
* [tl;dr](#tldr) 
* [Misc Notes](#misc-notes) 


Feats:
- Multi-node k8s cluster (1+ control, 2+ workers)
- HAProxy load balancer to distribute traffic to the k8s nodes
- Keepalived will run on both HAProxy VMs to manage a shared Virtual IP (VIP)
    - This will allow both HAProxy VMs to have the same IP

---

Maybe:
- SSL certs with Let's Encrypt


## Set up the k8s Environment
### Spin up VMs
Three to start:
* `control-node1`
* `worker-node1`
* `worker-node2`

To scale, you'd just create more worker nodes.  

The HAProxy nodes are separate.  
* `haproxy-lb1`
* `haproxy-lb2`

Add more load balancers to scale if more redundancy is needed.  


---

In a prod-like HA setup, the load balancers (HAProxy) typically run on separate hosts
from the worker nodes.  
If a worker node crashes or is busy, you don't want that to bring down the load
balancer.  
Dedicated load balancer VMs are easier to manage, configure, or reboot without
affecting the worker pods.  
With Keepalived, you want 2 distinct machines to have a failover mechanism for HA.  

### Install K8s
We need `kubeadm`, `kubelet`, `kubectl` on each node.  


### Initialize k8s Cluster
Initialize the k8s cluster on `control-node1`.  
```bash
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
```

---
More info:  
When you run `kubeadm init`, you use the `--pod-network-cidr` flag to specify the IP
address range for pods in the cluster.  
* `10.244.0.0/16` defines a network range from `10.244.0.0` up to `10.244.255.255`
* Typically choose a private, non-routable IP range to avoid conflicts with external
  networks.  
`10.244.0.0/16` is a common default used by certain Container Network Interface (CNI)
plugins like Flannel.  
You can technically pick another private range but most Flannel docs uses `10.244.0.0/16`.  

---

### Install a CNI plugin (flannel)


```bash
kubectl apply -f \
    https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
```

---
More info:  
A CNI plugin is responsible for setting up network interfaces in Linux containers and 
assigning IP addresses to those containers.  

In k8s, when pods come online the CNI plugin automatically creates network interfaces
and routes inside each node, as well as ensures that all pods can communicate with
each other (even across nodes) without requiring NAT.  

Flannel is one of the CNI plugin options. Some others are Calico, Weave Net, Cilium,
etc.  
It helps by providing the network overlay so that each pod gets its own unique IP
address within the `10.244.0.0/24` range.  
Without a CNI plugin, pods wouldn't be able to communicate across node boundaries seamlessly.  


### Join the Worker Nodes

SSH into each node and run the `kubeadm join` command that was generated by the 
control plane when the cluster was initialized with `kubeadm init`.  

Use `kubeadm join` to join the worker nodes.  
```bash
kubeadm join ...
```
* [next](#deploy-a-test-app)

---
More info:  
When initializing k8s with `kubeadm init`, near the end it prints instructions that
look something like:
```bash
kubeadm join 192.168.4.50:6443 --token abcdef.0123456789abcdef \
    --discovery-token-ca-cert-hash sha256:abcdef123456789
```
* `kubeadm join <api-server-ip>:6443`: Points the worker node to the control plane's API server.  
* `--token`: Secret token that authorizes the worker to join.  
* `--discovery-token-ca-cert-hash`: Ensures the node is connecting to the right
  master by verifying the CA cert hash.  


---
* SSH into each node and run the `kubeadm join` command that was generated by the control plane. 
* The node will contact the master, authenticate with the token and CA hash, then
  pull down the cluster config.  
* After a minute or two, the node should appear in `kubectl get nodes` on the control plane.  



### Deploy a Test App
Deploy a test app for the cluster to serve.  
```bash
kubectl create deployment nginx-demo --image=nginx:stable
kubectl scale deployment nginx-demo --replicas=2
kubectl expose deployment nginx-demo --type=NodePort --port=80
```

---

What these commands are doing:
* `kubectl create deployment nginx-demo`: Creates a `deployment` resource in k8s
  named `nginx-demo`.  
    * By default it uses the `nginx` image from DockerHub unless you specify another
      `--image`.  
    * Add `--image=nginx:stable` to make sure it's the right container image.  
* `kubectl scale deployment nginx-demo --replicas=2` 
    * Tells k8s to run 2 `replicas` (or copies) of the Pods managed by the
      `nginx-demo` deployment.
    * This ensures that if one Pod fails or one node goes down, there's another to
      handle requests.  
* `kubectl expose deployment nginx-demo --type=NodePort --port=80`
    * Creates a `service` of the type `NodePort`.  
    * This service listens on port 80 ***internally*** (the target port on the
      container) and exposes it on a high port on each node (the NodePort).  
    * This allows you to access the `nginx-demo` Pods from outside the cluster.  



### Check which NodePort was Assigned
So, when using `--type=NodePort`, you need to check which one was assigned.  
```bash
kubectl get svc nginx-demo
```

* `kubectl get <resource>` shows the current state of resources (pods, deployments, services)
* `get svc` is shorthand for "get services".  
    * This will list all Services in the current namespace, along with their cluster IP,
      external IP, ports, etc.  

You'll see a high number port, e.g., `30080`
Then reach the app at `<node-ip>:30080` 

---

In k8s, a NodePort is one of the ways to expose a service externally.  
This means that you can access the service from any node's IP, on that port, and k8s
will route the traffic to the correct Pod/Pods.


## Set up two HAProxy Nodes
We'll have two VMs, `haproxy-lb1` and `haproxy-lb2`.  

On both nodes, install `haproxy` with the package manager.  
```bash
# Debian-based:
sudo apt-get update && sudo apt-get install haproxy -y
# RedHat-based:
sudo dnf install haproxy -y
```

### Configure HAProxy 
Give HAProxy a minimal configuration in `/etc/haproxy/haproxy.cfg`
```cfg
global
    log /dev/log local0
    log /dev/log local1 notice
    chroot /var/lib/haproxy
    user haproxy
    group haproxy
    daemon

defaults
    log global
    mode http
    option httplog
    option dontlognull
    timeout connect 5s
    timeout client 30s
    timeout server 30s

frontend http_front
    bind *:80
    default_backend k8s_backend

backend k8s_backend
    balance roundrobin
    # Point to k8s nodes using the NodePort
    server worker1 192.168.4.67:30080 check
    server worker2 192.168.4.68:30080 check
```
* `frontend`: Receives incoming requests on port 80.  
* `backend`: Sends the requests to the kubernetes nodes on the NodePort.  
(change IP addresses accordingly)

Then restart HAProxy
```bash
sudo systemctl restart haproxy
sudo systemctl enable haproxy --now
```
* `--now` also tells it to `start` in addition to `enable`.  

At this point, each HAProxy node should be able to forward traffic independently to 
the k8s cluster.  

Now to set up Keepalived.  

## Set up Keepalived for a Virtual IP (VIP)
The goal here is to create a single IP address that automatically fails over between
the two HAProxy nodes (`haproxy-lb1` and `haproxy-lb2`).  

### Install Keepalived on the HAProxy Nodes
Install `keepalived` on both of the HAProxy nodes using the package manager.  
```bash
# Debian-based
sudo apt-get update && sudo apt-get install keepalived -y
# RedHat-based
sudo dnf install keepalived -y
```

### Configure Keepalived
The config file for keepalived is at `/etc/keepalived/keepalived.conf`.  

#### First HAProxy Node's Keepalived Configuration
```conf
vrrp_instance VI_1 {
    state MASTER            # MASTER on the primary node, BACKUP on the secondary
    interface ens18         # The network interface connected to the LAN (ip addr)
    virtual_router_id 51    # needs to be the same on both nodes
    priority 100            # Highest priority indicates the active MASTER node
    advert_int 1            # advertisement interval, in seconds
    authentication {
        auth_type PASS
        auth_pass secretpassword123
    }
    virtual_ipaddress {
        192.168.1.250/24   # adjust to match network
    }
}
```

* `state`: Set to `MASTER` on the primary node.  
* `interface`: The real network interface name (e.g., `eth0`, `ens3`, etc.).  
* `virtual_router_id`: Must match on both nodes. Can be any number between 0-255.  
* `priority`: The primary node should have a higher priority than the secondary node.  
* `virtual_ipaddress`: The actual virtual IP and subnet in CIDR notation.  
    * Pick an IP address that is in the same subnet as your network interface. 
        * e.g., `192.168.4.x/24`
    * This can't already be in use, and it should be reserved for this virtual IP.  
    * All the `keepalived` instances need to use the same IP in the config.  

#### Second HAProxy Node's Keepalived Configuration
Two things that need to be changed on the secondary node's configuration:  
* `state`: Needs to be `MASTER` on the primary node, `BACKUP` on the secondary node.  
* `priority`: Lower than the `MASTER`'s priority to make sure `MASTER` is active.  
Everything else should match the first node's config. Same `virtual_router_id`,
`auth_pass`, VIP.  
```conf
vrrp_instance VI_1 {
    state BACKUP            # MASTER on the primary node, BACKUP on the secondary
    interface ens18         # The network interface connected to the LAN (ip addr)
    virtual_router_id 51    # needs to be the same on both nodes
    priority 90             # Highest priority indicates the active MASTER node
    advert_int 1            # advertisement interval, in seconds
    authentication {
        auth_type PASS
        auth_pass secretpassword123
    }
    virtual_ipaddress {
        192.168.1.250/24   # adjust to match network
    }
}
```

### Start/Restart Keepalived
Hit `keepalived` with a `enable --now` on both of the HAProxy nodes to both enable
and start the service.  
```bash
sudo systemctl enable keepalived --now
```

---

At this point, the `haproxy-lb1` *should* hold the Virtual IP `192.168.1.250` (or whatever is specified in the `keepalived.conf` file).  

## Test the Virtual IP and Failover

* Ping the virtual IP to make sure it's responding.  
  ```bash
  ping -c 5 192.168.1.250
  ```

* Use `curl` on the VIP on port 80 to make sure it's responding.  
  ```bash
  curl http://192.168.1.250
  ```
    * This should show the nginx welcome page.  

* Simulate a failover by stopping `keepalived` on `haproxy-lb1`.  
  ```bash
  sudo systemctl stop keepalived
  ```
    * Wait a few seconds and then ping the IP again, and `curl` it again.  
    * The VIP should now move over to `haproxy-lb2`.  

* Fail back by restarting `keepalived` on the `haproxy-lb1` node.  
  ```bash
  sudo systemctl start keepalived
  ```
    * The VIP should fail back to the `haproxy-lb1` node since it has the 
      higher `priority` in its config.  


## tl;dr
1. Spin up a minimal Kubernetes cluster with 2–3 nodes.
2. Install HAProxy on two separate nodes/VMs. Configure each with the same backend servers pointing to your Kubernetes NodePorts.
3. Configure a VIP in `keepalived` (e.g., 192.168.1.250) so only the active (MASTER) HAProxy owns that IP at a time.
4. If the primary HAProxy goes down, Keepalived automatically assigns the VIP to the backup HAProxy, maintaining high availability.

## Misc Notes
* `SSL_ERROR_ZERO_RETURN`: The documentation specifies that `SSL_ERROR_ZERO_RETURN` is returned if
  the transport layer is closed normally

- In HAProxy you can enable the `stats` page to monitor traffic and do node healthchecks.  
- Make sure the NICs on HAProxy servers have static IP addresses so the VIP stays in
  the same subnet
